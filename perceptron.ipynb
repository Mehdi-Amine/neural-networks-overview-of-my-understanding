{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38064bit5f7c9fe68404493786e070ea1d1d2be7",
   "display_name": "Python 3.8.0 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron: My Understanding\n",
    "***\n",
    "## The Structure of a Perceptron\n",
    "An input layer fully connected with an output layer: \n",
    "- The input layer contains nodes representative of each attribute in the dataset. Think of each node matching one column in the dataset. *A common cause for confusion in my experience is to visualize the perceptron taking in more than one instance from the dataset at once.* That is not the case; the input layer is a placeholder for one instance at a time.\n",
    "\n",
    "- The output layer for perceptrons is one node. It contains the result of the dot product between the input and the parameters. Because dot products are linear functions, they are not enough to represent nonlinearities in the data. Neural Networks are called Universal Function Approximators because they can approximate any function. To be able to achieve this, non-linearity is introduced with activation functions like Sigmoid, and ReLU.\n",
    "\\begin{eqnarray} \n",
    "  a' = \\sigma(w a + b)\n",
    "\\tag{1}\\end{eqnarray}\n",
    "\n",
    "In equation 1, a is initially the input vector. a' is the vector of all the activations in the next layer. w is the vector of weights belonging to a'. Each node in a' has as many weights as there are nodes in a. b is the vector of biases belonging to a'. Each node in a' has only one bias. *A common confusion in my experience is to think of w and b as the parameters of a, while in fact they belong to a'.*\n",
    "\n",
    "**Note:** Sometimes the weights and biases are merged in one parameter vector. This can be done by prepending a 1 in the vector of inputs, and prepending the bias in the vector initially containing only the weights. \n",
    "\n",
    "## Training a Perceptron\n",
    "Initially, the parameters are generated randomly. A cost function is chosen to evaluate the perceptron's output.\n",
    "```python\n",
    "bias = numpy.random.randn(1, 1)\n",
    "weights = numpy.random.randn(input_length, 1) # for some integer input_length\n",
    "```  \n"
   ]
  }
 ]
}